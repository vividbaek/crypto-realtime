#!/bin/bash
# scripts/start-spark-job.sh
# Spark Job ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

set -e

cd /home/vividbaek/boaz

echo "=========================================="
echo "  âš¡ Spark Job ì‹¤í–‰"
echo "=========================================="

# Spark Master ì»¨í…Œì´ë„ˆ í™•ì¸
if ! docker ps --format "{{.Names}}" | grep -q "^spark-master$"; then
    echo "âŒ Spark Master ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "ë¨¼ì € ./scripts/start.shë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
    exit 1
fi

# Spark Master ë¡œê·¸ì—ì„œ ì‹œì‘ í™•ì¸
if ! docker-compose logs spark-master 2>&1 | grep -q "MasterWebUI.*started"; then
    echo "âš ï¸  Spark Masterê°€ ì•„ì§ ì‹œì‘ ì¤‘ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œ ëŒ€ê¸° ì¤‘..."
    sleep 5
fi

echo "âœ… Spark Master í™•ì¸ ì™„ë£Œ"
echo "  Master: http://localhost:8080"
echo ""

# Ivy ìºì‹œ ë””ë ‰í† ë¦¬ ì¤€ë¹„ (ë³¼ë¥¨ ë§ˆìš´íŠ¸ë¡œ í•´ê²°ë¨)
echo "ğŸ“¦ Ivy ìºì‹œ ë””ë ‰í† ë¦¬ ì¤€ë¹„ ì¤‘..."
# í˜¸ìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„± (ê¶Œí•œ ë¬¸ì œ í•´ê²°)
mkdir -p data/spark-ivy 2>/dev/null || true
# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ ì„œë¸Œë””ë ‰í† ë¦¬ ìƒì„± (ë³¼ë¥¨ì´ ë§ˆìš´íŠ¸ëœ í›„)
docker exec spark-master bash -c "mkdir -p /opt/spark/.ivy2/cache /opt/spark/.ivy2/jars && chmod -R 777 /opt/spark/.ivy2" 2>/dev/null || true

# Job íƒ€ì… ì„ íƒ
JOB_TYPE=${1:-depth}
JOB_FILE=""
JOB_NAME=""

case $JOB_TYPE in
    depth)
        JOB_FILE="kafka_reader.py"
        JOB_NAME="Depth Reader (í˜¸ê°€ ë°ì´í„°)"
        ;;
    aggtrade)
        JOB_FILE="aggtrade_processor.py"
        JOB_NAME="AggTrade Processor (1ë¶„ë´‰ ì§‘ê³„)"
        ;;
    *)
        echo "âŒ ì•Œ ìˆ˜ ì—†ëŠ” Job íƒ€ì…: $JOB_TYPE"
        echo ""
        echo "ì‚¬ìš©ë²•:"
        echo "  ./scripts/start-spark-job.sh [depth|aggtrade]"
        echo ""
        echo "ì˜ˆì‹œ:"
        echo "  ./scripts/start-spark-job.sh depth      # Depth í˜¸ê°€ ë°ì´í„° ì²˜ë¦¬"
        echo "  ./scripts/start-spark-job.sh aggtrade  # AggTrade 1ë¶„ë´‰ ì§‘ê³„"
        exit 1
        ;;
esac

echo "ğŸš€ Spark Job ì‹œì‘: $JOB_NAME"
echo "  íŒŒì¼: $JOB_FILE"
echo "  (Ctrl+Cë¡œ ì¤‘ì§€)"
echo ""

# Spark Job ì‹¤í–‰ (log4j ì„¤ì •ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ INFO ë¡œê·¸ ì œê±°)
docker exec -it spark-master bash -c "cd /opt/spark/work-dir && /opt/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1 --conf 'spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/opt/spark/work-dir/log4j.properties' --conf 'spark.executor.extraJavaOptions=-Dlog4j.configuration=file:/opt/spark/work-dir/log4j.properties' $JOB_FILE"
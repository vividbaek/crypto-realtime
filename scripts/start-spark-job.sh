#!/bin/bash
# scripts/start-spark-job.sh
# Spark Job ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

set -e

cd /home/vividbaek/boaz

echo "=========================================="
echo "  âš¡ Spark Job ì‹¤í–‰"
echo "=========================================="

# Spark Master ì»¨í…Œì´ë„ˆ í™•ì¸
if ! docker ps --format "{{.Names}}" | grep -q "^spark-master$"; then
    echo "âŒ Spark Master ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "ë¨¼ì € ./scripts/start.shë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
    exit 1
fi

# Spark Master ë¡œê·¸ì—ì„œ ì‹œìž‘ í™•ì¸
if ! docker-compose logs spark-master 2>&1 | grep -q "MasterWebUI.*started"; then
    echo "âš ï¸  Spark Masterê°€ ì•„ì§ ì‹œìž‘ ì¤‘ì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ìž ì‹œ ëŒ€ê¸° ì¤‘..."
    sleep 5
fi

echo "âœ… Spark Master í™•ì¸ ì™„ë£Œ"
echo "  Master: http://localhost:8080"
echo ""

# Ivy ìºì‹œ ë””ë ‰í† ë¦¬ ì¤€ë¹„ (ë³¼ë¥¨ ë§ˆìš´íŠ¸ë¡œ í•´ê²°ë¨)
echo "ðŸ“¦ Ivy ìºì‹œ ë””ë ‰í† ë¦¬ ì¤€ë¹„ ì¤‘..."
# í˜¸ìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„± (ê¶Œí•œ ë¬¸ì œ í•´ê²°)
mkdir -p data/spark-ivy 2>/dev/null || true
# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ ì„œë¸Œë””ë ‰í† ë¦¬ ìƒì„± (ë³¼ë¥¨ì´ ë§ˆìš´íŠ¸ëœ í›„)
docker exec spark-master bash -c "mkdir -p /opt/spark/.ivy2/cache /opt/spark/.ivy2/jars && chmod -R 777 /opt/spark/.ivy2" 2>/dev/null || true

echo "ðŸš€ Kafka Reader Job ì‹œìž‘..."
echo "  (Ctrl+Cë¡œ ì¤‘ì§€)"
echo ""

# Spark Job ì‹¤í–‰ (log4j ì„¤ì •ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ INFO ë¡œê·¸ ì œê±°)
docker exec -it spark-master bash -c "cd /opt/spark/work-dir && /opt/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1 --conf 'spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/opt/spark/work-dir/log4j.properties' --conf 'spark.executor.extraJavaOptions=-Dlog4j.configuration=file:/opt/spark/work-dir/log4j.properties' kafka_reader.py"
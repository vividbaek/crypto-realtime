#!/bin/bash
# scripts/start-spark-job.sh
# Spark Job ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
cd "$PROJECT_ROOT"

echo "=========================================="
echo "  âš¡ Spark Job ì‹¤í–‰"
echo "=========================================="

# Spark Master ì»¨í…Œì´ë„ˆ í™•ì¸
if ! docker ps --format "{{.Names}}" | grep -q "^spark-master$"; then
    echo "âŒ Spark Master ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "ë¨¼ì € ./scripts/start.shë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
    exit 1
fi

# Spark Master ë¡œê·¸ì—ì„œ ì‹œì‘ í™•ì¸
if ! docker-compose logs spark-master 2>&1 | grep -q "MasterWebUI.*started"; then
    echo "âš ï¸  Spark Masterê°€ ì•„ì§ ì‹œì‘ ì¤‘ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œ ëŒ€ê¸° ì¤‘..."
    sleep 5
fi

echo "âœ… Spark Master í™•ì¸ ì™„ë£Œ"
echo "  Master: http://localhost:8080"
echo ""

# Ivy ìºì‹œ ë””ë ‰í† ë¦¬ ì¤€ë¹„ (ë³¼ë¥¨ ë§ˆìš´íŠ¸ = data/spark-ivy â†’ /opt/spark/.ivy2)
echo "ğŸ“¦ Ivy ìºì‹œ ë””ë ‰í† ë¦¬ ì¤€ë¹„ ì¤‘..."
mkdir -p data/spark-ivy/cache data/spark-ivy/jars
chmod -R 777 data/spark-ivy 2>/dev/null || true
# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œë„ ê¶Œí•œ í™•ë³´ (ë§ˆìš´íŠ¸ëœ ê²½ë¡œ)
docker exec spark-master bash -c "mkdir -p /opt/spark/.ivy2/cache /opt/spark/.ivy2/jars && chmod -R 777 /opt/spark/.ivy2" 2>/dev/null || true

# ì¸ìë¡œ job ì„ íƒ (ê¸°ë³¸: kafka_reader / preprocess: 1ë¶„ë´‰ ì§‘ê³„ / kline: Binance 1ë¶„ë´‰)
JOB="${1:-kafka_reader}"
case "$JOB" in
  preprocess|stream_preprocess)
    JOB_SCRIPT="stream_preprocess.py"
    echo "ğŸš€ ì „ì²˜ë¦¬ Job ì‹œì‘ (aggTrade â†’ 1ë¶„ë´‰ ì§‘ê³„)..."
    ;;
  kline)
    JOB_SCRIPT="kline_console.py"
    echo "ğŸš€ Binance 1ë¶„ë´‰(kline_1m) ì½˜ì†” ì¶œë ¥ (ë¹„êµìš©)..."
    ;;
  *)
    JOB_SCRIPT="kafka_reader.py"
    echo "ğŸš€ Kafka Reader Job ì‹œì‘ (depth â†’ ì½˜ì†”)..."
    ;;
esac
echo "  (Ctrl+Cë¡œ ì¤‘ì§€)"
echo ""

# Spark Job ì‹¤í–‰ (log4j ì„¤ì •ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ INFO ë¡œê·¸ ì œê±°)
docker exec -it spark-master bash -c "cd /opt/spark/work-dir && /opt/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1 --conf 'spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/opt/spark/work-dir/log4j.properties' --conf 'spark.executor.extraJavaOptions=-Dlog4j.configuration=file:/opt/spark/work-dir/log4j.properties' $JOB_SCRIPT"
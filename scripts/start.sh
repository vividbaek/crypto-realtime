#!/bin/bash
# scripts/start.sh
# í”„ë¡œì íŠ¸ ì „ì²´ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸

set -e

# ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™ (ì–´ëŠ ì‚¬ìš©ì/ê²½ë¡œì—ì„œë„ ë™ì‘)
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
cd "$PROJECT_ROOT"

# --clean ì˜µì…˜ íŒŒì‹±
CLEAN_START=false
for arg in "$@"; do
    case $arg in
        --clean)
            CLEAN_START=true
            shift
            ;;
    esac
done

echo "=========================================="
echo "  ğŸš€ í”„ë¡œì íŠ¸ ì‹œì‘"
if [ "$CLEAN_START" = true ]; then
    echo "  ğŸ§¹ í´ë¦° ëª¨ë“œ (ëª¨ë“  ë°ì´í„° ì´ˆê¸°í™”)"
fi
echo "=========================================="

# 0. í´ë¦° ìŠ¤íƒ€íŠ¸: ê¸°ì¡´ ë°ì´í„° ì „ë¶€ ì‚­ì œ
if [ "$CLEAN_START" = true ]; then
    echo ""
    echo "ğŸ§¹ 0ë‹¨ê³„: í´ë¦° ìŠ¤íƒ€íŠ¸ - ê¸°ì¡´ ë°ì´í„° ì´ˆê¸°í™”..."
    
    # Docker ì„œë¹„ìŠ¤ ë¨¼ì € ì¤‘ì§€
    docker-compose down 2>/dev/null || true
    
    # Kafka ë°ì´í„° ì‚­ì œ
    rm -rf data/kafka/* 2>/dev/null || true
    echo "  âœ… Kafka ë°ì´í„° ì‚­ì œ"
    
    # ClickHouse ë°ì´í„° ì‚­ì œ (root ì†Œìœ  ê°€ëŠ¥)
    if [ -d "data/clickhouse" ] && [ "$(ls -A data/clickhouse 2>/dev/null)" ]; then
        docker run --rm -v "$(pwd)/data/clickhouse:/data" alpine sh -c "rm -rf /data/*" 2>/dev/null || rm -rf data/clickhouse/* 2>/dev/null || true
        echo "  âœ… ClickHouse ë°ì´í„° ì‚­ì œ"
    fi
    
    # Spark Ivy ìºì‹œ ì‚­ì œ (root ì†Œìœ  ê°€ëŠ¥)
    if [ -d "data/spark-ivy" ] && [ "$(ls -A data/spark-ivy 2>/dev/null)" ]; then
        docker run --rm -v "$(pwd)/data/spark-ivy:/data" alpine sh -c "rm -rf /data/*" 2>/dev/null || rm -rf data/spark-ivy/* 2>/dev/null || true
        echo "  âœ… Spark Ivy ìºì‹œ ì‚­ì œ"
    fi
    
    # Python ìºì‹œ ì‚­ì œ
    find . -type d -name "__pycache__" -not -path "./venv/*" -exec rm -rf {} + 2>/dev/null || true
    echo "  âœ… Python ìºì‹œ ì‚­ì œ"
    
    # Spark ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì´ë¯€ë¡œ downìœ¼ë¡œ ì¶©ë¶„)
    echo "  âœ… í´ë¦° ìŠ¤íƒ€íŠ¸ ì™„ë£Œ"
fi

# 1. ë°ì´í„° ë””ë ‰í„°ë¦¬ ì¤€ë¹„ (Kafka/ClickHouse/Spark ë³¼ë¥¨ì´ ì“¸ ìˆ˜ ìˆë„ë¡)
echo ""
echo "ğŸ“ ë°ì´í„° ë””ë ‰í„°ë¦¬ ì¤€ë¹„..."
mkdir -p data/kafka data/clickhouse data/spark-ivy
chmod 777 data/kafka data/clickhouse data/spark-ivy 2>/dev/null || true
echo "âœ… ë°ì´í„° ë””ë ‰í„°ë¦¬ ì¤€ë¹„ ì™„ë£Œ"

# 2. Docker ì„œë¹„ìŠ¤ ì‹œì‘
echo ""
echo "ğŸ“¦ 2ë‹¨ê³„: Docker ì„œë¹„ìŠ¤ ì‹œì‘..."
docker-compose up -d
echo "âœ… Docker ì„œë¹„ìŠ¤ ì‹œì‘ ì™„ë£Œ"

# í´ë¦° ìŠ¤íƒ€íŠ¸ ì‹œ Spark ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ (Kafka í† í”½ IDê°€ ë°”ë€Œë©´ Sparkê°€ ì˜ˆì „ IDë¡œ ìš”ì²­í•´ ë¸Œë¡œì»¤ ì—ëŸ¬ ë°œìƒ ë°©ì§€)
if [ "$CLEAN_START" = true ]; then
    echo "ğŸ§¹ Spark ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™” (í† í”½ ID ë³€ê²½ ëŒ€ë¹„)..."
    sleep 10
    docker exec spark-master rm -rf /tmp/checkpoint-* 2>/dev/null || true
    echo "  âœ… Spark ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ"
fi

# 3. Kafka ì¤€ë¹„ ëŒ€ê¸°
echo ""
echo "â³ 3ë‹¨ê³„: Kafka ì¤€ë¹„ ëŒ€ê¸° (30ì´ˆ)..."
sleep 30

# 2-1. Kafka ì—ëŸ¬ í™•ì¸ ë° ìë™ ë³µêµ¬
echo ""
echo "ğŸ” Kafka ìƒíƒœ í™•ì¸ ì¤‘..."

# Topic ID ë¶ˆì¼ì¹˜ ë˜ëŠ” Cluster ID ë¶ˆì¼ì¹˜ ê°ì§€
KAFKA_ERROR=$(docker-compose logs kafka 2>&1 | grep -iE "InconsistentClusterIdException|does not match the topic ID" | tail -1 || true)

if [ -n "$KAFKA_ERROR" ]; then
    echo "âš ï¸  Kafka ë°ì´í„° ë¶ˆì¼ì¹˜ ê°ì§€! ìë™ ë³µêµ¬ ì¤‘..."
    echo "   ì—ëŸ¬: $KAFKA_ERROR"
    
    # ì„œë¹„ìŠ¤ ì¤‘ì§€
    docker-compose stop kafka
    
    # Kafka ë°ì´í„° ì „ì²´ ì‚­ì œ (Topic ID ë¶ˆì¼ì¹˜ ë°©ì§€)
    rm -rf data/kafka/*
    echo "âœ… Kafka ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ"
    
    # ì¬ì‹œì‘
    docker-compose up -d kafka
    echo "âœ… Kafka ì¬ì‹œì‘ ì™„ë£Œ"
    
    # Spark ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ (í† í”½ IDê°€ ë°”ë€Œì—ˆìœ¼ë¯€ë¡œ ì˜ˆì „ ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš© ì‹œ ë¸Œë¡œì»¤ ì—ëŸ¬ ë°©ì§€)
    docker exec spark-master rm -rf /tmp/checkpoint-* 2>/dev/null || true
    echo "  âœ… Spark ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ"
    
    # ëŒ€ê¸°
    echo "â³ Kafka ì¬ì‹œì‘ ëŒ€ê¸° ì¤‘... (30ì´ˆ)"
    sleep 30
fi

# 3. í† í”½ ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ ê±´ë„ˆëœ€, ë‚´ë¶€ì—ì„œ ë¦¬ë” ì„ ì¶œ 15ì´ˆ ëŒ€ê¸° í¬í•¨)
echo ""
echo "ğŸ“ 4ë‹¨ê³„: Kafka í† í”½ í™•ì¸/ìƒì„±..."
./infra/setup-kafka.sh

# 4. ìƒíƒœ í™•ì¸
echo ""
echo "ğŸ“Š 5ë‹¨ê³„: ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"
docker-compose ps

# 5. Spark ìƒíƒœ í™•ì¸ ë° ì¤€ë¹„
echo ""
echo "ğŸ“Š 6ë‹¨ê³„: Spark ìƒíƒœ í™•ì¸ ë° ì¤€ë¹„..."
sleep 5

# Ivy ìºì‹œ ë””ë ‰í† ë¦¬ ì¤€ë¹„ (ë³¼ë¥¨ ë§ˆìš´íŠ¸ë¡œ í•´ê²°ë¨)
# í˜¸ìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p data/spark-ivy 2>/dev/null || true
# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ ì„œë¸Œë””ë ‰í† ë¦¬ ìƒì„± (ë³¼ë¥¨ì´ ë§ˆìš´íŠ¸ëœ í›„)
docker exec spark-master bash -c "mkdir -p /opt/spark/.ivy2/cache /opt/spark/.ivy2/jars && chmod -R 777 /opt/spark/.ivy2" 2>/dev/null || true

# Spark Master í™•ì¸ (ë¡œê·¸ ê¸°ë°˜)
if docker-compose logs spark-master 2>&1 | grep -q "MasterWebUI.*started"; then
    echo "âœ… Spark Master ì‹¤í–‰ ì¤‘ (http://localhost:8080)"
else
    echo "âš ï¸  Spark Master í™•ì¸ ì‹¤íŒ¨ (ë¡œê·¸ í™•ì¸ ì¤‘...)"
fi

# Spark Worker í™•ì¸ (ë¡œê·¸ ê¸°ë°˜)
WORKER_COUNT=$(docker-compose logs spark-master 2>&1 | grep -c "Registering worker" || echo "0")
if [ "$WORKER_COUNT" -gt "0" ]; then
    echo "âœ… Spark Worker ë“±ë¡ë¨: ${WORKER_COUNT}ê°œ"
else
    echo "âš ï¸  Spark Worker ë¯¸ë“±ë¡ (ì ì‹œ í›„ ìë™ ë“±ë¡ë  ìˆ˜ ìˆìŒ)"
fi

echo ""
echo "=========================================="
echo "  âœ… ì¤€ë¹„ ì™„ë£Œ!"
echo "=========================================="
echo ""
echo "ğŸ“¥ ë°ì´í„° ìˆ˜ì§‘ê¸° ì‹œì‘:"
echo "  source venv/bin/activate"
echo "  python3 -m collectors.bookticker_depth"
echo ""
echo "âš¡ Spark Job ì‹¤í–‰:"
echo "  ./scripts/start-spark-job.sh"
echo ""
echo "ğŸŒ Spark ì›¹ UI:"
echo "  http://localhost:8080"
echo ""
echo "ğŸ“Š ë©”ì‹œì§€ í™•ì¸:"
echo "  ./infra/manage-kafka.sh consume binance-depth 5"
# Binance Futures ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸

Binance ì„ ë¬¼ ê±°ë˜ì†Œì˜ ì‹¤ì‹œê°„ í˜¸ê°€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ , Kafkaë¥¼ í†µí•´ Sparkë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬í•˜ëŠ” ë°ì´í„° íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.

## ì•„í‚¤í…ì²˜

```
Binance WebSocket â”€â”€â†’ Python Collector â”€â”€â†’ Kafka â”€â”€â†’ Spark Streaming â”€â”€â†’ ClickHouse
   (ì‹¤ì‹œê°„ í˜¸ê°€)        (ë°ì´í„° ìˆ˜ì§‘)       (ë©”ì‹œì§€ í)    (ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬)       (ì €ì¥/ë¶„ì„)
```

| ì»´í¬ë„ŒíŠ¸ | ì—­í•  | ê¸°ìˆ  |
|---------|------|------|
| **Collector** | Binance WebSocketì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ â†’ Kafka ì „ì†¡ | Python, websockets, kafka-python |
| **Kafka** | ë©”ì‹œì§€ ë¸Œë¡œì»¤ (ë°ì´í„° ë²„í¼ë§ ë° ì „ë‹¬) | Confluent Kafka 7.3.0, ZooKeeper |
| **Spark** | Kafkaì—ì„œ Micro-Batchë¡œ ë°ì´í„°ë¥¼ ì½ì–´ íŒŒì‹±/ì§‘ê³„ | Spark 3.3.0 (Structured Streaming) |
| **ClickHouse** | ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ë° ë¶„ì„ ì¿¼ë¦¬ | ClickHouse (column-oriented DB) |

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
boaz/
â”œâ”€â”€ collectors/                  # ë°ì´í„° ìˆ˜ì§‘ê¸°
â”‚   â”œâ”€â”€ base_collector.py        #   WebSocket ì—°ê²° + Kafka ì „ì†¡ (ì¶”ìƒ í´ë˜ìŠ¤)
â”‚   â””â”€â”€ bookticker_depth.py      #   í˜¸ê°€ Depth ìˆ˜ì§‘ê¸°
â”œâ”€â”€ common/                      # ê³µí†µ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ config.py                #   ì„¤ì • (Kafka ì„œë²„, í† í”½ ë§¤í•‘)
â”‚   â””â”€â”€ kafka_utils.py           #   Kafka Producer ë˜í¼ (ì‹±ê¸€í†¤)
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ binance_stream_enum.py   #   Binance ìŠ¤íŠ¸ë¦¼ íƒ€ì… Enum
â”œâ”€â”€ spark_jobs/                  # Spark ì‘ì—…
â”‚   â”œâ”€â”€ kafka_reader.py          #   Kafka â†’ Spark ìŠ¤íŠ¸ë¦¬ë° ì½ê¸°/íŒŒì‹±
â”‚   â”œâ”€â”€ stream_aggregator.py     #   (ì˜ˆì •) 1ë¶„ë´‰ ì§‘ê³„
â”‚   â”œâ”€â”€ whale_detector.py        #   (ì˜ˆì •) ê³ ë˜ ê±°ë˜ ê°ì§€
â”‚   â””â”€â”€ log4j.properties         #   Spark ë¡œê·¸ ì„¤ì •
â”œâ”€â”€ infra/                       # ì¸í”„ë¼ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ setup-kafka.sh           #   Kafka í† í”½ ìƒì„± + ìƒíƒœ ê²€ì¦
â”‚   â””â”€â”€ manage-kafka.sh          #   Kafka ê´€ë¦¬ ë„êµ¬ (í† í”½ ì¡°íšŒ, ë©”ì‹œì§€ í™•ì¸ ë“±)
â”œâ”€â”€ database/
â”‚   â””â”€â”€ clickhouse_schema.sql    #   ClickHouse í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ
â”œâ”€â”€ scripts/                     # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ start.sh                 #   ì „ì²´ ì„œë¹„ìŠ¤ ì‹œì‘ (--clean ì˜µì…˜ ì§€ì›)
â”‚   â””â”€â”€ start-spark-job.sh       #   Spark Job ì‹¤í–‰
â”œâ”€â”€ tests/                       # Binance ìŠ¤íŠ¸ë¦¼ë³„ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ docker-compose.yml           # Docker ì„œë¹„ìŠ¤ ì •ì˜
â””â”€â”€ requirements.txt             # Python ì˜ì¡´ì„±
```

## ì‚¬ì „ ìš”êµ¬ì‚¬í•­

- **Docker** & **Docker Compose**
- **Python 3.10+**

## ë¹ ë¥¸ ì‹œì‘

### 1. Python ê°€ìƒí™˜ê²½ ì„¤ì •

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. ì „ì²´ ì„œë¹„ìŠ¤ ì‹œì‘

```bash
# ì¼ë°˜ ì‹œì‘ (Docker ì„œë¹„ìŠ¤ + Kafka í† í”½ ìƒì„± + Spark ì¤€ë¹„)
./scripts/start.sh

# í´ë¦° ì‹œì‘ (ëª¨ë“  ë°ì´í„° ì´ˆê¸°í™” í›„ ì‹œì‘) â† ë¬¸ì œ ë°œìƒ ì‹œ ê¶Œì¥
./scripts/start.sh --clean
```

ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…:
1. Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (Kafka, ZooKeeper, Spark, ClickHouse)
2. Kafka ì¤€ë¹„ ëŒ€ê¸° ë° ìƒíƒœ ê²€ì¦ (Topic ID ë¶ˆì¼ì¹˜ ìë™ ë³µêµ¬)
3. Kafka í† í”½ ìƒì„± (`binance-depth`)
4. Spark Master/Worker ìƒíƒœ í™•ì¸

### 3. ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘

```bash
source venv/bin/activate
python3 -m collectors.bookticker_depth
```

**Depth + 1ë¶„ë´‰ + aggTrade ë™ì‹œ ìˆ˜ì§‘** (1ë¶„ë´‰Â·agg ë°ì´í„° í™•ì¸ìš©):

```bash
python3 -m collectors.depth_kline_aggtrade
```

- `binance-depth`: í˜¸ê°€ (ê³ ë¹ˆë„)
- `binance-kline`: Binance 1ë¶„ë´‰ (ì´ë¯¸ 1ë¶„ ì§‘ê³„)
- `binance-trade`: aggTrade ì²´ê²°

ë©”ì‹œì§€ í™•ì¸: `./infra/manage-kafka.sh consume binance-kline 3`, `./infra/manage-kafka.sh consume binance-trade 3`

ì •ìƒ ë™ì‘ ì‹œ ì¶œë ¥:
```
ğŸš€ BookTickerDepthCollector ì‹œì‘ | êµ¬ë…: [<BinanceStreamType.DEPTH: 'depth@100ms'>]
ğŸ“¥ [11:17:14] btcusdt@depth@100ms ìƒ˜í”Œ ë°ì´í„° í™•ì¸
â±ï¸ TPS: 9.07 msgs/sec | ëˆ„ì : 200
```

### 4. Spark Job ì‹¤í–‰

ìƒˆ í„°ë¯¸ë„ì„ ì—´ê³ :
```bash
./scripts/start-spark-job.sh
```

ì •ìƒ ë™ì‘ ì‹œ ì¶œë ¥:
```
+-------+---------+---------+--------------------+
| symbol|bid_price|ask_price|     kafka_timestamp|
+-------+---------+---------+--------------------+
|BTCUSDT|  68970.0|  68971.3|2026-02-11 02:18:...|
+-------+---------+---------+--------------------+
```

## ë°ì´í„° íë¦„ ìƒì„¸

### Collector â†’ Kafka

1. `BookTickerDepthCollector`ê°€ Binance Futures WebSocket (`wss://fstream.binance.com`)ì— ì—°ê²°
2. `btcusdt@depth@100ms` ìŠ¤íŠ¸ë¦¼ êµ¬ë… (100ms ê°„ê²© í˜¸ê°€ ë³€ê²½ ë°ì´í„°)
3. ìˆ˜ì‹  ë°ì´í„°ë¥¼ JSON ì§ë ¬í™”í•˜ì—¬ `binance-depth` í† í”½ìœ¼ë¡œ ì „ì†¡
4. ë°°ì¹˜ ìµœì í™”: `batch_size=32KB`, `linger_ms=10`, `gzip` ì••ì¶•

Kafka ë©”ì‹œì§€ í˜•ì‹:
```json
{
  "symbol": "BTCUSDT",
  "stream": "btcusdt@depth@100ms",
  "data": {
    "e": "depthUpdate",
    "b": [["68970.00", "1.500"], ...],
    "a": [["68971.30", "2.000"], ...]
  },
  "ts": 1739233095000
}
```

### Kafka â†’ Spark

**ê¸°ë³¸ (depth â†’ ì½˜ì†”):**
```bash
./scripts/start-spark-job.sh
```
- `binance-depth` êµ¬ë… â†’ bid/ask íŒŒì‹± â†’ 1ì´ˆë§ˆë‹¤ ì½˜ì†” ì¶œë ¥

**ì „ì²˜ë¦¬ (aggTrade â†’ 1ë¶„ë´‰ ì§‘ê³„):**
```bash
./scripts/start-spark-job.sh preprocess
```
- `binance-trade`(aggTrade) êµ¬ë… â†’ 1ë¶„ tumbling windowë¡œ OHLCV ì§‘ê³„ â†’ 1ë¶„ë§ˆë‹¤ ì½˜ì†” ì¶œë ¥ (ì´í›„ ClickHouse ì ì¬ í™•ì¥ ê°€ëŠ¥)

**ë°ì´í„° ì´ˆê¸°í™” í›„ 1ë¶„ë´‰ ë¹„êµ (ìš°ë¦¬ ì§‘ê³„ vs Binance 1ë¶„ë´‰):**
1. `./scripts/start.sh --clean` â€” KafkaÂ·Spark ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™”
2. í„°ë¯¸ë„ 1: `python3 -m collectors.depth_kline_aggtrade` â€” ìŠ¤íŠ¸ë¦¼ ìˆ˜ì§‘ â†’ Kafka ì ì¬
3. í„°ë¯¸ë„ 2: `./scripts/start-spark-job.sh preprocess` â€” ì´ˆë‹¹(aggTrade) ë°ì´í„°ë¡œ 1ë¶„ë´‰ ì •ì œ ì¶œë ¥
4. í„°ë¯¸ë„ 3: `python3 scripts/compare_binance_kline.py` â€” Kafkaì˜ Binance 1ë¶„ë´‰(kline_1m)ì„ ë´‰ ë‹«í ë•Œë§Œ ì¶œë ¥

ê°™ì€ `window_start`(ë¶„) ê¸°ì¤€ìœ¼ë¡œ í„°ë¯¸ë„ 2(ìš°ë¦¬ 1ë¶„ë´‰)ì™€ í„°ë¯¸ë„ 3(Binance 1ë¶„ë´‰) ìˆ«ìë¥¼ ë¹„êµí•˜ë©´ ì „ì²˜ë¦¬ ê²€ì¦ ê°€ëŠ¥.

## Kafka ê´€ë¦¬ ë„êµ¬

```bash
# í† í”½ ëª©ë¡ ì¡°íšŒ
./infra/manage-kafka.sh list

# í† í”½ ìƒì„¸ ì •ë³´ (íŒŒí‹°ì…˜, ë¦¬ë”, ISR)
./infra/manage-kafka.sh describe binance-depth

# ë©”ì‹œì§€ ìˆ˜ì‹  í™•ì¸ (ìµœê·¼ 5ê°œ)
./infra/manage-kafka.sh consume binance-depth 5

# í† í”½ ì˜¤í”„ì…‹ ì •ë³´
./infra/manage-kafka.sh offsets binance-depth

# Consumer Group Lag í™•ì¸
./infra/manage-kafka.sh lag
```

## ì›¹ UI

| ì„œë¹„ìŠ¤ | URL |
|--------|-----|
| Spark Master | http://localhost:8080 |
| ClickHouse HTTP | http://localhost:8123 |

## Docker ì„œë¹„ìŠ¤

| ì„œë¹„ìŠ¤ | ì´ë¯¸ì§€ | í¬íŠ¸ |
|--------|--------|------|
| ZooKeeper | confluentinc/cp-zookeeper:7.3.0 | 2181 (ë‚´ë¶€) |
| Kafka | confluentinc/cp-kafka:7.3.0 | 9092 (ì™¸ë¶€), 29092 (ë‚´ë¶€) |
| Spark Master | apache/spark-py:v3.3.0 | 8080, 7077 |
| Spark Worker | apache/spark-py:v3.3.0 | - |
| ClickHouse | clickhouse/clickhouse-server:latest | 8123, 9000 |

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Spark submit ì‹œ Ivy FileNotFoundException (`.ivy2/cache/...`)

**ì›ì¸**: Ivy ìºì‹œ ë””ë ‰í„°ë¦¬(`data/spark-ivy`)ê°€ ì—†ê±°ë‚˜ ì»¨í…Œì´ë„ˆì—ì„œ ì“¸ ìˆ˜ ì—†ìŒ.

**í•´ê²°**:
```bash
mkdir -p data/spark-ivy/cache data/spark-ivy/jars
chmod -R 777 data/spark-ivy
./scripts/start-spark-job.sh preprocess
```
ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´: `sudo chmod -R 777 data/spark-ivy` ë˜ëŠ” `rm -rf data/spark-ivy` í›„ ë‹¤ì‹œ ì‹¤í–‰.

### Kafka `NotLeaderForPartitionError`

**ì›ì¸**: í† í”½ì„ ì‚­ì œ/ì¬ìƒì„±í–ˆì„ ë•Œ `data/kafka/`ì— ì´ì „ Topic ID ë¡œê·¸ê°€ ë‚¨ì•„ìˆìœ¼ë©´ ë°œìƒ

**í•´ê²°**:
```bash
./scripts/start.sh --clean
```

### Sparkì—ì„œ ë°ì´í„°ë¥¼ ëª» ì½ëŠ” ê²½ìš°

1. Collectorê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
2. Kafkaì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸: `./infra/manage-kafka.sh consume binance-depth 3`
3. ë°ì´í„°ê°€ ì—†ìœ¼ë©´ Kafka ìƒíƒœ í™•ì¸: `./infra/manage-kafka.sh describe binance-depth`

### ë„ì»¤ ì¬ì‹œì‘ í›„ "í† í”½ì´ ì•ˆ ë§ì•„ì„œ" / ë¸Œë¡œì»¤ ìª½ ì—ëŸ¬ (Topic ID ë¶ˆì¼ì¹˜)

**ì›ì¸**: í† í”½ ì´ë¦„Â·ë¸Œë¡œì»¤ ì£¼ì†ŒëŠ” ì½”ë“œ/ì„¤ì •ìœ¼ë¡œ ê³ ì •ë˜ì–´ **ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ë°”ë€Œì§€ ì•ŠìŠµë‹ˆë‹¤.**  
ë‹¤ë§Œ `--clean` ë˜ëŠ” `data/kafka` ì‚­ì œ í›„ Kafkaë¥¼ ë‹¤ì‹œ ë„ìš°ë©´ **í† í”½ì´ ìƒˆ IDë¡œ ìƒì„±**ë˜ê³ , Spark ì²´í¬í¬ì¸íŠ¸ì—ëŠ” **ì˜ˆì „ í† í”½ ID**ê°€ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤. Sparkê°€ ê·¸ ì²´í¬í¬ì¸íŠ¸ë¡œ ë¸Œë¡œì»¤ì— ìš”ì²­í•˜ë©´ ë¸Œë¡œì»¤ê°€ "topic ID does not match"ë¡œ ê±°ì ˆí•©ë‹ˆë‹¤.

**í•´ê²°**:

- **`./scripts/start.sh --clean`** ì‚¬ìš© ì‹œ: Spark ì²´í¬í¬ì¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì‚­ì œí•˜ë„ë¡ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ì¬ì‹œì‘ í›„ ë‹¤ì‹œ ìˆ˜ì§‘ê¸°Â·Spark Jobë§Œ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤.
- **ìˆ˜ë™ìœ¼ë¡œ Kafkaë§Œ ì´ˆê¸°í™”í•œ ê²½ìš°**: Spark ì²´í¬í¬ì¸íŠ¸ë¥¼ ì§€ìš´ ë’¤ Spark Jobì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.
  ```bash
  docker exec spark-master rm -rf /tmp/checkpoint-*
  ./scripts/start-spark-job.sh
  ```

### ì„œë¹„ìŠ¤ ì „ì²´ ì¬ì‹œì‘

```bash
docker-compose down
./scripts/start.sh --clean
```

# spark_jobs/aggtrade_processor.py
"""
Binance Aggregate Trade ë°ì´í„° ì²˜ë¦¬ ë° 1ë¶„ë´‰ ì§‘ê³„

Kafka binance-aggtrade í† í”½ì—ì„œ aggTrade ë°ì´í„°ë¥¼ ì½ì–´
1ë¶„ ë‹¨ìœ„ë¡œ OHLCV(ì‹œê°€/ê³ ê°€/ì €ê°€/ì¢…ê°€/ê±°ë˜ëŸ‰) ìº”ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, get_json_object, from_unixtime, window,
    first, last, max as spark_max, min as spark_min, sum as spark_sum, count
)
import time


def create_spark_session(app_name="AggTradeProcessor"):
    """Spark ì„¸ì…˜ ìƒì„±"""
    return SparkSession.builder \
        .appName(app_name) \
        .config("spark.jars.packages", 
                "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1") \
        .config("spark.sql.streaming.checkpointLocation", f"/tmp/checkpoint-{app_name}") \
        .config("spark.driver.extraJavaOptions", "-Dlog4j.configuration=file:/opt/spark/work-dir/log4j.properties") \
        .config("spark.executor.extraJavaOptions", "-Dlog4j.configuration=file:/opt/spark/work-dir/log4j.properties") \
        .getOrCreate()


def read_from_kafka(spark, topic="binance-aggtrade", starting_offsets="latest"):
    """Kafkaì—ì„œ aggTrade ë°ì´í„° ì½ê¸°"""
    return spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "kafka:29092") \
        .option("subscribe", topic) \
        .option("startingOffsets", starting_offsets) \
        .option("kafka.session.timeout.ms", "60000") \
        .option("kafka.request.timeout.ms", "90000") \
        .option("kafka.max.poll.interval.ms", "300000") \
        .option("kafka.heartbeat.interval.ms", "10000") \
        .option("kafka.metadata.max.age.ms", "300000") \
        .option("kafka.reconnect.backoff.ms", "50") \
        .option("kafka.reconnect.backoff.max.ms", "1000") \
        .option("failOnDataLoss", "false") \
        .option("maxOffsetsPerTrigger", "1000") \
        .load()


def parse_aggtrade_data(df):
    """
    aggTrade ë°ì´í„° íŒŒì‹±
    
    ë°ì´í„° êµ¬ì¡°:
    {
      "symbol": "BTCUSDT",
      "data": {
        "e": "aggTrade",
        "s": "BTCUSDT", 
        "a": 5933014,       // Aggregate trade ID
        "p": "68970.50",    // Price
        "q": "1.234",       // Quantity
        "f": 100,           // First trade ID
        "l": 105,           // Last trade ID
        "T": 1739561025000, // Trade time (ms)
        "m": true           // Is buyer maker?
      }
    }
    """
    return df.select(
        get_json_object(col("value").cast("string"), "$.symbol").alias("symbol"),
        get_json_object(col("value").cast("string"), "$.data.a").cast("long").alias("trade_id"),
        get_json_object(col("value").cast("string"), "$.data.p").cast("double").alias("price"),
        get_json_object(col("value").cast("string"), "$.data.q").cast("double").alias("quantity"),
        (get_json_object(col("value").cast("string"), "$.data.T").cast("long") / 1000).cast("timestamp").alias("trade_time"),
        get_json_object(col("value").cast("string"), "$.data.m").cast("boolean").alias("is_buyer_maker"),
        col("timestamp").alias("kafka_timestamp")
    )


def aggregate_to_1min_candle(parsed_df):
    """
    1ë¶„ë´‰ ìº”ë“¤ ìƒì„± (OHLCV)
    
    Returns:
        DataFrame with columns:
        - window_start: ìº”ë“¤ ì‹œì‘ ì‹œê°
        - window_end: ìº”ë“¤ ì¢…ë£Œ ì‹œê°
        - symbol: ì‹¬ë³¼
        - open: ì‹œê°€ (ì²« ê±°ë˜ ê°€ê²©)
        - high: ê³ ê°€
        - low: ì €ê°€
        - close: ì¢…ê°€ (ë§ˆì§€ë§‰ ê±°ë˜ ê°€ê²©)
        - volume: ê±°ë˜ëŸ‰
        - trades: ê±°ë˜ íšŸìˆ˜
        - buy_volume: ë§¤ìˆ˜ ê±°ë˜ëŸ‰ (taker buy)
        - sell_volume: ë§¤ë„ ê±°ë˜ëŸ‰ (taker sell)
    """
    # 1ë¶„ ìœˆë„ìš°ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì§‘ê³„
    candle_df = parsed_df \
        .withWatermark("trade_time", "1 minute") \
        .groupBy(
            window(col("trade_time"), "1 minute"),
            col("symbol")
        ) \
        .agg(
            first("price").alias("open"),
            spark_max("price").alias("high"),
            spark_min("price").alias("low"),
            last("price").alias("close"),
            spark_sum("quantity").alias("volume"),
            count("*").alias("trades"),
            spark_sum(
                col("quantity").cast("double") * (1 - col("is_buyer_maker").cast("int"))
            ).alias("buy_volume"),
            spark_sum(
                col("quantity").cast("double") * col("is_buyer_maker").cast("int")
            ).alias("sell_volume")
        ) \
        .select(
            col("window.start").alias("window_start"),
            col("window.end").alias("window_end"),
            col("symbol"),
            col("open").cast("double"),
            col("high").cast("double"),
            col("low").cast("double"),
            col("close").cast("double"),
            col("volume").cast("double"),
            col("trades").cast("long"),
            col("buy_volume").cast("double"),
            col("sell_volume").cast("double")
        )
    
    return candle_df


def main():
    print("=" * 80)
    print("  ğŸš€ Binance AggTrade Processor")
    print("=" * 80)
    print("  ğŸ“Š ê¸°ëŠ¥: aggTrade â†’ 1ë¶„ë´‰ OHLCV ì§‘ê³„")
    print("  ğŸ“¥ ì…ë ¥: Kafka binance-aggtrade í† í”½")
    print("  ğŸ“¤ ì¶œë ¥: ì½˜ì†” (1ë¶„ë§ˆë‹¤)")
    print("=" * 80)
    print()
    
    # Spark ì„¸ì…˜ ìƒì„±
    spark = create_spark_session("AggTradeProcessor")
    
    # Kafka Consumer Group Coordinator ì´ˆê¸°í™” ëŒ€ê¸°
    print("â³ Kafka Consumer Group Coordinator ì´ˆê¸°í™” ëŒ€ê¸° ì¤‘... (30ì´ˆ)")
    time.sleep(30)
    
    # Kafkaì—ì„œ ë°ì´í„° ì½ê¸°
    print("ğŸ“¥ Kafkaì—ì„œ aggTrade ë°ì´í„° ì½ê¸° ì‹œì‘...")
    kafka_df = read_from_kafka(spark, "binance-aggtrade", starting_offsets="earliest")
    
    # aggTrade ë°ì´í„° íŒŒì‹±
    print("ğŸ” aggTrade ë°ì´í„° íŒŒì‹± ì¤‘...")
    parsed_df = parse_aggtrade_data(kafka_df)
    
    # 1ë¶„ë´‰ ìº”ë“¤ ì§‘ê³„
    print("ğŸ“Š 1ë¶„ë´‰ ìº”ë“¤ ì§‘ê³„ ì‹œì‘...")
    candle_df = aggregate_to_1min_candle(parsed_df)
    
    # ê²°ê³¼ ì¶œë ¥ (1ë¶„ë§ˆë‹¤ ë°°ì¹˜ ì²˜ë¦¬)
    print("ğŸš€ ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì‹œì‘... (1ë¶„ë§ˆë‹¤ ìº”ë“¤ ì¶œë ¥)")
    print("=" * 80)
    print()
    
    query = candle_df.writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", "false") \
        .trigger(processingTime='10 seconds') \
        .start()
    
    query.awaitTermination()


if __name__ == "__main__":
    main()

# spark_jobs/kafka_reader.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, get_json_object

def create_spark_session(app_name="BinanceProcessor"):
    """Spark ì„¸ì…˜ ìƒì„± (ì¬ì‚¬ìš© ê°€ëŠ¥)"""
    return SparkSession.builder \
        .appName(app_name) \
        .config("spark.jars.packages", 
                "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1") \
        .config("spark.sql.streaming.checkpointLocation", f"/tmp/checkpoint-{app_name}") \
        .config("spark.driver.extraJavaOptions", "-Dlog4j.configuration=file:/opt/spark/work-dir/log4j.properties") \
        .config("spark.executor.extraJavaOptions", "-Dlog4j.configuration=file:/opt/spark/work-dir/log4j.properties") \
        .getOrCreate()

def read_from_kafka(spark, topic, starting_offsets="latest"):
    """Kafkaì—ì„œ ë°ì´í„° ì½ê¸° (ì¬ì‚¬ìš© ê°€ëŠ¥)"""
    return spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "kafka:29092") \
        .option("subscribe", topic) \
        .option("startingOffsets", starting_offsets) \
        .option("kafka.session.timeout.ms", "60000") \
        .option("kafka.request.timeout.ms", "90000") \
        .option("kafka.max.poll.interval.ms", "300000") \
        .option("kafka.heartbeat.interval.ms", "10000") \
        .option("kafka.metadata.max.age.ms", "300000") \
        .option("kafka.reconnect.backoff.ms", "50") \
        .option("kafka.reconnect.backoff.max.ms", "1000") \
        .option("failOnDataLoss", "false") \
        .option("maxOffsetsPerTrigger", "1000") \
        .load()

def parse_depth_data(df):
    """Depth ë°ì´í„° íŒŒì‹± (ì¬ì‚¬ìš© ê°€ëŠ¥)"""
    return df.select(
        get_json_object(col("value").cast("string"), "$.symbol").alias("symbol"),
        get_json_object(col("value").cast("string"), "$.data.b[0][0]").cast("double").alias("bid_price"),
        get_json_object(col("value").cast("string"), "$.data.a[0][0]").cast("double").alias("ask_price"),
        col("timestamp").alias("kafka_timestamp")
    )

def main():
    import time
    
    spark = create_spark_session("BinanceDepthReader")
    
    # Kafkaê°€ ì™„ì „íˆ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸° (Coordinator ì´ˆê¸°í™” ëŒ€ê¸°)
    print("â³ Kafka Consumer Group Coordinator ì´ˆê¸°í™” ëŒ€ê¸° ì¤‘... (30ì´ˆ)")
    time.sleep(30)
    
    # Kafka ì½ê¸° (earliestë¡œ ë³€ê²½í•˜ì—¬ ê¸°ì¡´ ë°ì´í„°ë„ ì½ê¸°)
    print("ğŸ“¥ Kafkaì—ì„œ ë°ì´í„° ì½ê¸° ì‹œì‘...")
    kafka_df = read_from_kafka(spark, "binance-depth", starting_offsets="earliest")
    
    # íŒŒì‹±
    parsed_df = parse_depth_data(kafka_df)
    
    # ì¶œë ¥ (1ì´ˆë§ˆë‹¤ ë°°ì¹˜ ì²˜ë¦¬)
    print("ğŸš€ ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì‹œì‘... (1ì´ˆë§ˆë‹¤ ë°°ì¹˜ ì²˜ë¦¬)")
    query = parsed_df.writeStream \
        .outputMode("append") \
        .format("console") \
        .trigger(processingTime='1 second') \
        .start()
    
    query.awaitTermination()

if __name__ == "__main__":
    main()
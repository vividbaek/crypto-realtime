# spark_jobs/kafka_reader.py
"""
ì‚¬ì‹¤ìƒ kafkaì˜ í•¨ìˆ˜ ëª¨ìŒ ì§‘(ë°ì´í„° ì½ê³  ë¶ˆëŸ¬ì˜¤ëŠ” ìš©ë„)
ë‹¨ë…ìœ¼ë¡œ ì‹¤í–‰í•  ë•ŒëŠ” depth í† í”½ì„ 1ì´ˆë§ˆë‹¤ ì½˜ì†”ì— ì°ëŠ” í…ŒìŠ¤íŠ¸/í™•ì¸ìš©
stream_preprocess.py ê°™ì€ ì „ì²˜ë¦¬ jobì´ ì—¬ê¸°ì„œ create, read, parseë“± importí•´ì„œ ì‚¬ìš©
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, get_json_object

# ìŠ¤íŒŒí¬ ì‘ì—…ì„ ì‹œì‘í•˜ê¸° ìœ„í•œ "í™˜ê²½ ì„¤ì •"
def create_spark_session(app_name="BinanceProcessor"):
    """
    Spark ì„¸ì…˜ ìƒì„± (ì¬ì‚¬ìš© ê°€ëŠ¥)
    appnameì€ í† í”½ì„
    spark.jars.pacages: ì¹´í”„ì¹´ ì „ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜´
    checkpointLocation: ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—ëŸ¬ ë‚¬ì„ ë•Œ, ê¸°ë¡í•˜ëŠ” ê²ƒ
    log4j.properties: í•„ìš”í•œ ë¡œê·¸ë§Œ ë³´ë ¤ê³  ì •ë¦¬í•¨.
    """
    return SparkSession.builder \
        .appName(app_name) \
        .config("spark.jars.packages", 
                "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1") \
        .config("spark.sql.streaming.checkpointLocation", f"/tmp/checkpoint-{app_name}") \
        .config("spark.driver.extraJavaOptions", "-Dlog4j.configuration=file:/opt/spark/work-dir/log4j.properties") \
        .config("spark.executor.extraJavaOptions", "-Dlog4j.configuration=file:/opt/spark/work-dir/log4j.properties") \
        .getOrCreate()

def read_from_kafka(spark, topic, starting_offsets="latest"):
    """
    Kafkaì—ì„œ ë°ì´í„° ì½ê¸° (ì¬ì‚¬ìš© ê°€ëŠ¥)
    kafka.bootstrap.servers: kafka29092ë¼ëŠ” ì£¼ì†Œë¡œ ì ‘ì†
    subscribe: ì¸ìë¡œ ë°›ì€ topic êµ¬ë…
    startingOffsets: latestëŠ” ì§€ê¸ˆë¶€í„°, earlistëŠ” ê³¼ê±° ë°ì´í„°ë¶€í„° ë‹¤ ê°€ì ¸ì˜¤ê² ë‹¤ëŠ” ê²ƒ
    failOnDataLoss: ë°ì´í„°ê°€ ì¼ë¶€ ì—†ì–´ë„ ë©ˆì¶”ì§€ ë§ê³  ê³„ì† ì§„í–‰(ì•ˆì „ì¥ì¹˜)
    maxOffsetsPerTrigger: í•œë²ˆì— ë„ˆë¬´ ë§ì´ ê°€ì ¸ì˜¤ë©´ ë ‰ ê±¸ë¦¬ë‹ˆ 1,000ê°œì”©ìœ¼ë¡œ ì œí•œ
    """
    return spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "kafka:29092") \
        .option("subscribe", topic) \
        .option("startingOffsets", starting_offsets) \
        .option("kafka.session.timeout.ms", "60000") \
        .option("kafka.request.timeout.ms", "90000") \
        .option("kafka.max.poll.interval.ms", "300000") \
        .option("kafka.heartbeat.interval.ms", "10000") \
        .option("kafka.metadata.max.age.ms", "300000") \
        .option("kafka.reconnect.backoff.ms", "50") \
        .option("kafka.reconnect.backoff.max.ms", "1000") \
        .option("failOnDataLoss", "false") \
        .option("maxOffsetsPerTrigger", "1000") \
        .load()

###### ì¹´í”„ì¹´ì—ì„œ ì˜¨ ë°ì´í„°ëŠ” valueë¼ëŠ” ì»¬ëŸ¼ ì•ˆì— ëª¨ë“  ë‚´ìš©ì´ JSON ë¬¸ìì—´ë¡œ ìˆìŒ (íŒŒì‹±í•´ì•¼í•¨) ####
def parse_depth_data(df):
    """
    Depth ë°ì´í„° íŒŒì‹± (ì¬ì‚¬ìš© ê°€ëŠ¥)
    ë°”ì´ë‚¸ìŠ¤ì˜ Depth ë°ì´í„°ì—ì„œ ë§¤ìˆ˜/ë§¤ë„ 1í˜¸ê°€ ê°€ê²©(bid_price, ask_price)ë§Œ ê°€ì ¸ì˜´
    $data,b[0][0] JSON êµ¬ì¡° ì•ˆì—ì„œ ìœ„ì¹˜ë¥¼ ì°¾ì•„ê°ê°
    """
    return df.select(
        get_json_object(col("value").cast("string"), "$.symbol").alias("symbol"),
        get_json_object(col("value").cast("string"), "$.data.b[0][0]").cast("double").alias("bid_price"),
        get_json_object(col("value").cast("string"), "$.data.a[0][0]").cast("double").alias("ask_price"),
        col("timestamp").alias("kafka_timestamp")
    )


def parse_trade_data(df):
    """
    aggTrade ë°ì´í„° íŒŒì‹± (ì¬ì‚¬ìš© ê°€ëŠ¥). ì²´ê²°ê°€/ìˆ˜ëŸ‰/ì‹œê° ì¶”ì¶œ.
    ì‹¤ì‹œê°„ ì²´ê²° ë‚´ì—­(agggTrade)ì²˜ë¦¬
    ë°”ì´ë‚¸ìŠ¤ì—ì„œëŠ” ì‹œê°„ì„ ë°€ë¦¬ì´ˆ(ms)ë¡œ ì¤Œ. ìœˆë„ìš° ì§‘ê³„ë¥¼ ìœ„í•´ 1,000ìš°ë¡œ ë‚˜ëˆ ì²˜ ì´ˆë¡œ ë³€ê²½
    .catst(string) ì¹´í”„ì¹´ëŠ” ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë³´ê´€í•˜ê¸° ìœ„í•´ ì´ì§„ìˆ˜ í˜•íƒœë¡œ ì €ì¥(binary -> stringìœ¼ë¡œ ë³€ê²½)
    aliasëŠ” ë³„ì¹­
    """
    v = col("value").cast("string")
    return df.select(
        get_json_object(v, "$.symbol").alias("symbol"),
        (get_json_object(v, "$.data.p").cast("double")).alias("price"),
        (get_json_object(v, "$.data.q").cast("double")).alias("quantity"),
        (get_json_object(v, "$.data.T").cast("long") / 1000).alias("event_time_sec"),  # ì´ˆ ë‹¨ìœ„ë¡œ ìœˆë„ìš°ìš©
        get_json_object(v, "$.data.m").cast("boolean").alias("is_buyer_maker"),
    )


def parse_kline_data(df):
    """Kline(1ë¶„ë´‰) ë°ì´í„° íŒŒì‹±. Binanceê°€ ì´ë¯¸ 1ë¶„ ì§‘ê³„í•œ ê°’."""
    v = col("value").cast("string")
    return df.select(
        get_json_object(v, "$.symbol").alias("symbol"),
        (get_json_object(v, "$.data.k.t").cast("long") / 1000).alias("window_start_sec"),
        get_json_object(v, "$.data.k.o").cast("double").alias("open"),
        get_json_object(v, "$.data.k.h").cast("double").alias("high"),
        get_json_object(v, "$.data.k.l").cast("double").alias("low"),
        get_json_object(v, "$.data.k.c").cast("double").alias("close"),
        get_json_object(v, "$.data.k.v").cast("double").alias("volume"),
        get_json_object(v, "$.data.k.n").cast("long").alias("trades"),
        get_json_object(v, "$.data.k.x").cast("boolean").alias("is_candle_closed"),
    )

def main():
    import time
    
    spark = create_spark_session("BinanceDepthReader")
    
    # KafkaëŠ” ì´ë¯¸ ì‹¤í–‰ ì¤‘. Consumer ì—°ê²° ì „ ì§§ì€ ëŒ€ê¸° (coordinator ëŒ€ë¹„)
    print("â³ Kafka ì—°ê²° ì „ ëŒ€ê¸° (5ì´ˆ)...")
    time.sleep(5)
    
    # Kafka ì½ê¸° (earliestë¡œ ë³€ê²½í•˜ì—¬ ê¸°ì¡´ ë°ì´í„°ë„ ì½ê¸°)
    print("ğŸ“¥ Kafkaì—ì„œ ë°ì´í„° ì½ê¸° ì‹œì‘...")
    kafka_df = read_from_kafka(spark, "binance-depth", starting_offsets="earliest")
    
    # íŒŒì‹±
    parsed_df = parse_depth_data(kafka_df)
    
    # ì¶œë ¥ (1ì´ˆë§ˆë‹¤ ë°°ì¹˜ ì²˜ë¦¬)
    print("ğŸš€ ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì‹œì‘... (1ì´ˆë§ˆë‹¤ ë°°ì¹˜ ì²˜ë¦¬)")
    query = parsed_df.writeStream \
        .outputMode("append") \
        .format("console") \
        .trigger(processingTime='1 second') \
        .start()
    
    query.awaitTermination()

if __name__ == "__main__":
    main()